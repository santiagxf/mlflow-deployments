{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Effortless models deployment withÂ MLflow\r\n",
        "\r\n",
        "## Working with partitioned models (many models) in MLflow\r\n",
        "\r\n",
        "This example demonstrates how to package multiple models with MLflow to work all together to produce inference over partitioned data. In this scenario, the data can be partitioned based on a given attribute and then individual models are trained for each slice of the data. This is a typical approach in time series data when training individual models for each partition outperforms training a larger model over the entire dataset. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with the M5 forecasting dataset\r\n",
        "\r\n",
        "To demonstrate this scenario, we are going to solve the [M5 Forecasting competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy/overview) problem which tries to predict the demand of diffent products on different retail stores across the US. Instead of training 1 model over the entire dataset, we are going to partition the data by store and then train one model per each store. In this dataset, there are 10 different stores so this will produce 10 different models.\r\n",
        "\r\n",
        "At the end, we will package all those 10 models into a single entity that can be deploy using MLflow."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import gc\r\n",
        "from typing import List, Dict, Any, Union, Tuple"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600597050
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by reading the input data from the 3 CSV files:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales = pd.read_csv('data/sales_train_validation.csv')\r\n",
        "calendar = pd.read_csv('data/calendar.csv')\r\n",
        "prices = pd.read_csv('data/sell_prices.csv')"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600605591
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining multiple datasets into one"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function that can take the three datasets and build a unified one:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(sales, calendar, prices, remove_old: bool = True):\r\n",
        "    data = sales\r\n",
        "    data.drop(columns=['id'], inplace=True)\r\n",
        "    calendar.drop(columns=['weekday', 'year'], inplace=True)\r\n",
        "\r\n",
        "    data = pd.melt(data, id_vars = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\r\n",
        "\r\n",
        "    data = pd.merge(data, calendar, how='left', left_on=['day'], right_on=['d'])\r\n",
        "    data.drop(columns=['day', 'd'], inplace=True)\r\n",
        "\r\n",
        "    data = data.merge(prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\r\n",
        "    data.drop(columns=['wm_yr_wk'], inplace=True)\r\n",
        "\r\n",
        "    if remove_old:\r\n",
        "        del sales\r\n",
        "        del calendar\r\n",
        "        del prices\r\n",
        "        gc.collect()\r\n",
        "\r\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600605753
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `build_dataset` will construct our final dataset. We then we delete the old datasets to save memory in the compute we are using as this data can grow large."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = build_dataset(sales, calendar, prices)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600693551
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and validation split"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will separate our data in train and validation. Notice that this function doesn't return the splits of the data but the row indices selected for each data set. Since the data is big, you will notice across all this notebook that we will index the data instead of making copies of it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test_idxs(data, cutoff_date):\r\n",
        "    data.sort_values('date', inplace=True)\r\n",
        "\r\n",
        "    train_idxs = data['date'] <= cutoff_date\r\n",
        "    valid_idxs = data['date'] > cutoff_date\r\n",
        "\r\n",
        "    data.drop(columns=['date'], inplace=True)\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    return train_idxs, valid_idxs"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600693753
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply the function to generate the splits. We are considering anything before 2014-04-24 as part of the training data:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_idxs, valid_idxs = split_train_test_idxs(data, cutoff_date='2014-04-24')"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600760982
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering\r\n",
        "\r\n",
        "We are going to separate our target variable from the features:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, target = data.loc[:, data.columns != \"demand\"], data[[\"demand\"]]\r\n",
        "\r\n",
        "del data\r\n",
        "gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600784473
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's build now a function that can pre-process the input features. To keep it simple, we are only going to focus on doing categorical encoding of the variables and handling missing values. The function will also return the transformation that can be use to transform data later. We will put this transformation inside of our final model:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "def preprocess(data: pd.DataFrame, categorical_features: List[str]) -> Tuple[pd.DataFrame, ColumnTransformer]:\r\n",
        "    \"\"\"\r\n",
        "    Preprocess the input data features.\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    data: pd.DataFrame\r\n",
        "        The input data features\r\n",
        "    categorical_features: List[str]\r\n",
        "        The features that you want to treat as categorical\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    Tuple[pd.DataFrame, ColumnTransformer]:\r\n",
        "        The transformed data and the associated transformations.\r\n",
        "    \"\"\"\r\n",
        "    transformations = ColumnTransformer(\r\n",
        "            [\r\n",
        "                ('encoding', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=-1), categorical_features),\r\n",
        "            ],\r\n",
        "            remainder='passthrough'\r\n",
        "        )\r\n",
        "\r\n",
        "    transformed = transformations.fit_transform(data)\r\n",
        "    columns = [name.split('__')[1] for name in transformations.get_feature_names_out()]\r\n",
        "\r\n",
        "    return pd.DataFrame(data=transformed, columns=columns, index=data.index).infer_objects(), transformations"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600785068
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it over the features:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\r\n",
        "features, transforms = preprocess(features, categorical_features)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603032605
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparating the training framework"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to generate a training function. The following training function trans a LightGBM model for the given datasets. It receives the input data (both containing training and validation), and the train-tests indices within this dataset. That means that indices in `train_idxs` will be used for training while `valid_idxs` will be used for validation. The argument `params` will be used to pass the hyperparameters of the model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\r\n",
        "from sklearn import metrics\r\n",
        "from typing import List, Dict, Any\r\n",
        "from mlflow.models.signature import infer_signature\r\n",
        "import lightgbm as lgb\r\n",
        "\r\n",
        "def train_and_evaluate(features: pd.DataFrame, target: pd.DataFrame, train_idxs: pd.Index, valid_idxs: pd.Index,\r\n",
        "                       params: Dict[str, Any], model_name: str = None) -> Tuple[lgb.Booster, float]:\r\n",
        "    \"\"\"\r\n",
        "    Trains a model using a LightGBM.\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    features: pd.DataFrame\r\n",
        "        The input features, already preprocessed.\r\n",
        "    target: pd.DataFrame\r\n",
        "        The column to predict.\r\n",
        "    train_idxs: pd.Index\r\n",
        "        The indices of the data correspoding to training.\r\n",
        "    valid_idxs: pd.Index\r\n",
        "        The indices of the data corresponding to validation.\r\n",
        "    params: Dict[str, Any]:\r\n",
        "        The training parameters for the model.\r\n",
        "    metrics_prefix: Union[str, None]\r\n",
        "        A prefix to be used for logging metrics.\r\n",
        "    \"\"\"\r\n",
        "    X_train, X_valid = features[train_idxs], features[valid_idxs]\r\n",
        "    y_train, y_valid = target[train_idxs], target[valid_idxs]\r\n",
        "\r\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train)\r\n",
        "    dvalid = lgb.Dataset(X_valid, label=y_valid)\r\n",
        "\r\n",
        "    clf = lgb.train(params, dtrain, 2500, valid_sets = [dtrain, dvalid], callbacks=[lgb.early_stopping(stopping_rounds=10)])\r\n",
        "\r\n",
        "    y_pred_valid = clf.predict(X_valid, num_iteration=clf.best_iteration)\r\n",
        "\r\n",
        "    val_error = np.sqrt(metrics.mean_squared_error(y_valid, y_pred_valid))\r\n",
        "    val_score = metrics.r2_score(y_valid, y_pred_valid)\r\n",
        "\r\n",
        "    mlflow.log_metric(\"valid_rmse\", val_error)\r\n",
        "    mlflow.log_metric(\"valid_r2\", val_score)\r\n",
        "\r\n",
        "    if model_name:\r\n",
        "        mlflow.lightgbm.log_model(clf, model_name, signature=infer_signature(features, target))\r\n",
        "    \r\n",
        "    return (clf, val_error)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603033858
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For sake of simplicity, the hyper-parameters we will use will be the same:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\r\n",
        "    'num_leaves': 500,\r\n",
        "    'min_child_weight': 0.034,\r\n",
        "    'feature_fraction': 0.379,\r\n",
        "    'bagging_fraction': 0.418,\r\n",
        "    'min_data_in_leaf': 106,\r\n",
        "    'objective': 'regression',\r\n",
        "    'max_depth': -1,\r\n",
        "    'learning_rate': 0.005,\r\n",
        "    \"boosting_type\": \"gbdt\",\r\n",
        "    \"bagging_seed\": 42,\r\n",
        "    \"metric\": 'rmse',\r\n",
        "    \"verbosity\": -1,\r\n",
        "    'reg_alpha': 0.3899,\r\n",
        "    'reg_lambda': 0.648,\r\n",
        "    'random_state': 222,\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603034582
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to need another helper function, which will return the `store_id` associated with a given \"categorical encoding\" of this column. Since our feature dataset is preprocess, the store \"CA_1\" is encoded as a numeric value. The function then will return for an input `0` the associated label, for instance `CA_1`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_encoded_value(transformations: ColumnTransformer, column: str, value: float) -> str:\r\n",
        "    \"\"\"\r\n",
        "    Returns the label associated with a given encoded value for a given column.\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    transformations: ColumnTransformer\r\n",
        "        The transformations used to encode the columns.\r\n",
        "    column: str\r\n",
        "        The name of the column\r\n",
        "    value: float\r\n",
        "        The encoded value of the column indicated.\r\n",
        "    \"\"\"\r\n",
        "    column_encoder_idxs = [i for i in range(len(transformations.transformers_)) if column in transformations.transformers_[i][2]]\r\n",
        "    if column_encoder_idxs:\r\n",
        "        column_encoder_idx = column_encoder_idxs[0]\r\n",
        "        column_index = transformations.transformers_[column_encoder_idx][2].index(column)\r\n",
        "        return transformations.transformers_[column_encoder_idx][1].categories_[column_index][int(value)]\r\n",
        "    else:\r\n",
        "        raise ValueError(f'There is no transformation applied to column ``{column}``')"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603035294
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to design our model. We want to create a partitioned multi-model, meaning, the data will be partitioned by a given key (in this case \"store_id\") and we will train one model per each of those partitions. When running inference, we want to apply the correct model based on the partition the data belongs to.\r\n",
        "\r\n",
        "To solve this problem we will create a custom model in MLflow. The `PartitionedModelEnsemble` will be a meta model that will be able to apply multiple models to a given input data set based on a partition key. Those models can be trained independently or all together, like in this example.\r\n",
        "\r\n",
        "> Tip: Notice how models are loaded using `mlflow.pyfunc.load_model`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.pyfunc import PythonModel, PythonModelContext\r\n",
        "from typing import Dict\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "import pandas as pd\r\n",
        "import mlflow\r\n",
        "\r\n",
        "class PartitionedModelEnsemble(PythonModel):\r\n",
        "    \"\"\"\r\n",
        "    A custom model that implements a paritioned inferrencing strategy over an ensamble of models.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, key: str, prediction_col: str, transformations: ColumnTransformer):\r\n",
        "        \"\"\"\r\n",
        "        Creates a new instance of the mode.\r\n",
        "\r\n",
        "        Parameters\r\n",
        "        ----------\r\n",
        "        key: str\r\n",
        "            The name of the column the data will be partitioned on.\r\n",
        "        prediction_col: str\r\n",
        "            The name of the column the model generates predictions on.\r\n",
        "        transformations: ColumnTransformer\r\n",
        "            Any given transformation that needs to be applied to the data before sending to the model.\r\n",
        "        \"\"\"\r\n",
        "        self.pred_col = prediction_col\r\n",
        "        self.key = key\r\n",
        "        self.transformations = transformations\r\n",
        "\r\n",
        "    def load_context(self, context: PythonModelContext):\r\n",
        "        \"\"\"\r\n",
        "        Loads all the models for the given partitions. This method assumes the models were logged with the\r\n",
        "        different of the column `key` as artifact key.\r\n",
        "        \"\"\"\r\n",
        "        self.models = { key: mlflow.pyfunc.load_model(model_path) for key, model_path in context.artifacts.items() }\r\n",
        "        \r\n",
        "    def predict(self, context: PythonModelContext, data: pd.DataFrame) -> pd.DataFrame:\r\n",
        "        if isinstance(data, pd.DataFrame):\r\n",
        "            predictions = pd.DataFrame(0, index=data.index, columns=[self.pred_col])\r\n",
        "            \r\n",
        "            # Get all the unique partition's value in the input data\r\n",
        "            key_ids = data[self.key].unique()\r\n",
        "\r\n",
        "            # We will run 1 predict call per each partition\r\n",
        "            for key_id in key_ids:\r\n",
        "                input_data_idx = data[self.key] == key_id\r\n",
        "\r\n",
        "                if self.transformations:\r\n",
        "                    columns = [name.split('__')[1] for name in self.transformations.get_feature_names_out()]\r\n",
        "                    transformed_data = pd.DataFrame(self.transformations.transform(input_data[input_data_idx]), columns=columns)\r\n",
        "                else:\r\n",
        "                    transformed_data = input_data[input_data_idx]\r\n",
        "                predictions[input_data_idx] = self.models[key_id].predict(transformed_data.loc[:, transformed_data.columns != self.key]).reshape(-1,1)\r\n",
        "\r\n",
        "            return predictions\r\n",
        "        \r\n",
        "        raise TypeError(\"This implementation can only take pandas DataFrame as inputs\")"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603036379
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model ensemble training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time now to train our model. Let's start by configuring the run with MLflow:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(experiment_name=\"m5-forecasting-mlflow\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "<Experiment: artifact_location='', creation_time=1687962235375, experiment_id='a0ca7d8e-43e1-4d8f-aa9e-7eb636b58fc7', last_update_time=None, lifecycle_stage='active', name='m5-forecasting-mlflow', tags={}>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688603037578
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following training routine which will train all the models, one per each partition. It does the following:\r\n",
        "\r\n",
        "1. It gets all the different values for the stores in the dataset. There are 10 different stores.\r\n",
        "2. For each of them:\r\n",
        "\r\n",
        "    a. We start a new child run in MLflow. We will have one child run per each store. This makes comparation and evaluation much easier. \r\n",
        "    b. Selects the indices of the data that belong to this particular store. This is done for training and validation datasets.\r\n",
        "    c. Calls the `train_and_evaluate()` function passing as arguments the features (all of them but the partition key column), the training indices and the test indices for the given store.\r\n",
        "    d. The function returns the trained model and the associated performance of it. Autolog is taking care of all the logging so we don't do much here.\r\n",
        "    e. In a dictionary called `artifacts`, we will record which is the artifact associated with the given store. The artifact is indicated in the form `runs:/<RUN_ID>/<ARTIFACT_PATH`. The reason for that is that this makes very clear from where the associated model came from, which we can use for lineage purposes later."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\r\n",
        "\r\n",
        "with mlflow.start_run() as run:\r\n",
        "    models = {}\r\n",
        "    model_error = {}\r\n",
        "    artifacts = {}\r\n",
        "\r\n",
        "    partition_key = 'store_id'\r\n",
        "    model_features = features.columns.difference([partition_key]).values\r\n",
        "    stores = features[partition_key].unique()\r\n",
        "\r\n",
        "    mlflow.lightgbm.autolog(log_datasets=False, log_models=False)\r\n",
        "\r\n",
        "    for store in tqdm(stores):\r\n",
        "        store_id = get_encoded_value(transforms, partition_key, store)\r\n",
        "        with mlflow.start_run(run_name=f\"train_store_{store_id}\", nested=True) as store_run:\r\n",
        "            store_train_idx = (train_idxs) & (features[partition_key] == store)\r\n",
        "            store_valid_idx = (valid_idxs) & (features[partition_key] == store)\r\n",
        "            models[store_id], model_error[store_id] = train_and_evaluate(features=features.loc[:, features.columns != partition_key],\r\n",
        "                                                                         target=target, \r\n",
        "                                                                         train_idxs=store_train_idx, \r\n",
        "                                                                         valid_idxs=store_valid_idx,\r\n",
        "                                                                         params=params,\r\n",
        "                                                                         model_name=store_id)\r\n",
        "\r\n",
        "            artifacts[store_id] = f\"runs:/{store_run.info.run_id}/{store_id}\"\r\n",
        "\r\n",
        "    mlflow.pyfunc.log_model(\"model\", \r\n",
        "                            python_model=PartitionedModelEnsemble(partition_key, target, transforms),\r\n",
        "                            artifacts=artifacts,\r\n",
        "                            registered_model_name=\"m5-forcasting-partitioned\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "  0%|          | 0/10 [00:00<?, ?it/s]2023/07/05 23:51:34 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n100%|ââââââââââ| 10/10 [56:54<00:00, 341.50s/it]\n2023/07/06 00:45:42 INFO mlflow.types.utils: Unsupported type hint: <class 'pandas.core.frame.DataFrame'>, skipping schema inference\n2023/07/06 00:45:42 INFO mlflow.types.utils: Unsupported type hint: <class 'pandas.core.frame.DataFrame'>, skipping schema inference\nRegistered model 'm5-forcasting-partitioned' already exists. Creating a new version of this model...\n2023/07/06 00:46:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: m5-forcasting-partitioned, version 2\nCreated version '2' of model 'm5-forcasting-partitioned'.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[762]\ttraining's rmse: 3.34108\tvalid_1's rmse: 3.28104\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[1063]\ttraining's rmse: 2.72586\tvalid_1's rmse: 3.22566\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[1119]\ttraining's rmse: 1.76246\tvalid_1's rmse: 2.21233\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[762]\ttraining's rmse: 3.49055\tvalid_1's rmse: 3.46766\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[415]\ttraining's rmse: 3.63284\tvalid_1's rmse: 2.89144\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[1245]\ttraining's rmse: 2.80333\tvalid_1's rmse: 3.46991\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[661]\ttraining's rmse: 2.69411\tvalid_1's rmse: 2.66093\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[550]\ttraining's rmse: 2.26246\tvalid_1's rmse: 2.50658\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[762]\ttraining's rmse: 5.06703\tvalid_1's rmse: 4.67845\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[676]\ttraining's rmse: 1.63436\tvalid_1's rmse: 1.73468\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688604403625
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that at the end of the routine, we are logging a new model in a very particular way. We are logging an instance of our custom model, `PartitionedModelEnsemble`. Pay closer attention about how artifacts is indicated. Artifacts are used to indicate all the models this aggregator model ensembles together. When artifacts are indicated in the form of `runs:/`, MLflow will automatically pull those artifacts from the MLflow server and log them inside of the model as a single unit. So you still have a single unit and reproducible asset that you can move along. However, the advantage of doing it this way is that you can easily see from where these models came from."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model\r\n",
        "\r\n",
        "Let's see now how this model works in practice when running inference. Let's start by pulling the model from the registry:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = mlflow.pyfunc.load_model(f\"models:/m5-forcasting-partitioned/latest\")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600360825
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a sample dataset we can use to test this out:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales = pd.read_csv('data/sales_train_validation.csv')\r\n",
        "calendar = pd.read_csv('data/calendar.csv')\r\n",
        "prices = pd.read_csv('data/sell_prices.csv')\r\n",
        "data = build_dataset(sales, calendar, prices).sample(n=1000)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600480806
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's remove the target column:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=[\"demand\"], inplace=True)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600481022
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the input data. We will create a copy of it:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = data.copy().reset_index()"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600481171
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(input_data)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "MlflowException",
          "evalue": "Failed to convert column item_id from type object to DataType.double.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2363\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"FOODS_2_268\"",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/santiagxf-cpu/code/repos/mlflow/mlflow/models/utils.py:420\u001b[0m, in \u001b[0;36m_enforce_mlflow_datatype\u001b[0;34m(name, values, t)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/tools/numeric.py:185\u001b[0m, in \u001b[0;36mto_numeric\u001b[0;34m(arg, errors, downcast)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     values, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_numeric\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2405\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"FOODS_2_268\" at position 0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
            "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/santiagxf-cpu/code/repos/mlflow/mlflow/pyfunc/__init__.py:416\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    414\u001b[0m input_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_input_schema()\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;129;01mand\u001b[39;00m MLFLOW_OPENAI_RETRIES_ENABLED\u001b[38;5;241m.\u001b[39mget():\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openai_auto_retry_patch\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/santiagxf-cpu/code/repos/mlflow/mlflow/models/utils.py:749\u001b[0m, in \u001b[0;36m_enforce_schema\u001b[0;34m(pf_input, input_schema)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _enforce_tensor_schema(pf_input, input_schema)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_schema\u001b[38;5;241m.\u001b[39mhas_input_names():\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_enforce_named_col_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpf_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _enforce_unnamed_col_schema(pf_input, input_schema)\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/santiagxf-cpu/code/repos/mlflow/mlflow/models/utils.py:487\u001b[0m, in \u001b[0;36m_enforce_named_col_schema\u001b[0;34m(pf_input, input_schema)\u001b[0m\n\u001b[1;32m    485\u001b[0m new_pf_input \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m required_input_names:\n\u001b[0;32m--> 487\u001b[0m     new_pf_input[x] \u001b[38;5;241m=\u001b[39m \u001b[43m_enforce_mlflow_datatype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpf_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Upstream validation means that if we're here, pf_input can only be a\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# pandas dataframe or a dict.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m optional_input_names \u001b[38;5;241m=\u001b[39m input_schema\u001b[38;5;241m.\u001b[39moptional_input_names()\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/santiagxf-cpu/code/repos/mlflow/mlflow/models/utils.py:422\u001b[0m, in \u001b[0;36m_enforce_mlflow_datatype\u001b[0;34m(name, values, t)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mto_numeric(values, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 422\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    423\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert column \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m from type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, values\u001b[38;5;241m.\u001b[39mdtype, t)\n\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    426\u001b[0m numpy_type \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m numpy_type\u001b[38;5;241m.\u001b[39mkind:\n",
            "\u001b[0;31mMlflowException\u001b[0m: Failed to convert column item_id from type object to DataType.double."
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688600482279
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model._model_impl.python_model.models[\"CA_1\"].metadata.signature"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "inputs: \n  ['item_id': double, 'dept_id': double, 'cat_id': double, 'store_id': double, 'state_id': double, 'event_name_1': double, 'event_type_1': double, 'event_name_2': double, 'event_type_2': double, 'wday': double, 'month': double, 'snap_CA': double, 'snap_TX': double, 'snap_WI': double, 'sell_price': double]\noutputs: \n  [Tensor('float64', (-1,))]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1688447808463
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en",
        "ms_ignore_dictionary": [
          "huggingface"
        ]
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}