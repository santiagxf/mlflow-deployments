{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effortless models deployment withÂ Mlflow\n",
    "\n",
    "## Packaging an NLP text review classifier from HuggingFace with Mlflow\n",
    "\n",
    "This example demostrates how to package models with Mlflow that require multiple assets to be loaded on inference time. To showcase the case, I will try to show an example as close as possible to real life: let's try to save an NLP classifier created with the popular library transformers from HuggingFace. This model will classify reviews according to a 5 stars ranking: 1, 2, 3, 4 or 5. We will create the model and then show how you can save it in MLFlow format to then achieved our so-called effortless deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model from HuggingFace\n",
    "\n",
    "Let's try to create an NLP classifier that assing the number of stars associated with a given text representing a product review. We are going to borrow a model already trained to perform this task from HuggingFace. HuggingFaceðŸ¤— is one of the most robust AI communities out there, with a wide range of solutions from models to datasets, built on top of open source principles, so let's take advantage of it.\n",
    "\n",
    "In this case we will use [`nlptown/bert-base-multilingual-uncased-sentiment`](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you). This is a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).\n",
    "\n",
    "The model can be used directly as a sentiment analysis model for product reviews in any of the six languages, or further finetuned on related sentiment analysis tasks. To keep the example small, we won't do any fine-tunning with our own data in this opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.auto import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "Let's start by loading our model configuration. To do that we will use the library `transformers` which provides a convenient way to pull a model from the HuggingFace repository just by using it's URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = 'nlptown/bert-base-multilingual-uncased-sentiment'\n",
    "config = AutoConfig.from_pretrained(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see some interesting properties of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Architecture:', config.architectures)\n",
    "print('Classes:', config.label2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the aspects that make BERT-based models to perform well is the used of well designed tokenizers. Tokenizer will allow us to transform the text from sequence of characters to sequences of words or tokens (actually, BERT uses piece-wise tokenizers, so it will return sequencies of parts of words). Tokenizers are an important concept cause you have to ensure you use the same tokenizer you model was trained with. Fortunately, `transformers` have a convenient way to pull tokenizers associated with a given model easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_uri)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_uri, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Switching model to CUDA device\")\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    print(\"No CUDA device found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't do any further training, so it is important to switch our model to evaluation mode so we get reproducible predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model with some sample data. To do that, we can create a sample text to send to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = pd.DataFrame({ 'text': ['good enough',\n",
    "                                 'The overall quality if good, but there are certain aspects of the product that made it hard to use']})\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our model. Our model can't handle text directly, which is why we need a tokenizer. It will convert the text to tensors representing the text. Then we can pass those representations to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(list(sample['text'].values), padding=True, return_tensors='pt')\n",
    "\n",
    "if model.device.index != None:\n",
    "    print(\"Model is in a different device as inputs. Moving location to device:\", model.device.index)\n",
    "    for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].to(model.device.index)\n",
    "    \n",
    "predictions = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model actually returns the log of the probabilities, so we need to change the domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "probs = torch.nn.Softmax(dim=1)(predictions.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using PyTorch backend with `transformers`, which will return tensors in the training/inference device. To easily manipulate them, we can move them to a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = probs.argmax(axis=1)\n",
    "confidences = probs.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame({ 'rating': [config.id2label[c] for c in classes], 'confidence': confidences })\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, our model looks to work good. It would be nice to have a validation dataset to actually measure how good or bad our model performs. I will let that as an exercise for the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model with Mlflow\n",
    "\n",
    "Now that we are fine with the model we got, it's time to save it. As usual, the first step it to create the model signature. Let's see what are the inputs an outputs of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "signature = infer_signature(sample, outputs)\n",
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a HuggingFace model with Mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mlflow doesn't support directly HuggingFace models, so we have to use the flavor `pyfunc` to save it. As we did in the previous example with the recommender model, we can create a Python class that inherits from `PythonModel` and then place everthing we need there. Something like this:\n",
    "\n",
    "```python\n",
    "class BertTextClassifier(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def predict(self, context: mlflow.pyfunc.PythonContext, data):\n",
    "        (...)\n",
    "```\n",
    "\n",
    "Althought it works, doing so would have some limitations:\n",
    "\n",
    "- `model` and `tokenizer` will get serialized in the object, but PyTorch has more efficient ways to store models.\n",
    "- `model` contains references to the training device and hence those will get serialized too\n",
    "- `model` is a big object, so persisting it will generate a big `Pickle` file.\n",
    "\n",
    "However, Mlflow provides another way to deal with artifacts that you model may need to opperate but that you don't want to serialize in a Python object. That is done by indicating `artifacts`.\n",
    "\n",
    "#### Artifacts in Mlflow\n",
    "\n",
    "We didn't mentioned before, but if you pay closer look to the signature of the method `mlflow.pyfunc.log_model` you will find an argument called `artifacts`. This parameter can be used to indicate any artifact (meaning, any file) that need to be packaged in the model package. It can be 1) any number of files and 2) of any type. Whatever you indicate there will be persisted and packaged along with the model object.\n",
    "\n",
    "Artifacts are indicated using a dictionary with keys as the name of the artifact, and value as the path in the local file system where the artifact is currently placed. **Any file indicated in this dictionary will be copied and packaged inside the package along with the model.** Note that artifacts are always path to files, it can't be a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` library provides a convenient way to store all the artifacts of a given model, and that is using the function`save_pretrained` from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'rating_classifier'\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a single file called `pytorch_model.bin` which contains the weights of the model itself. However, remember that in order to run the model we also need it's corresponding tokenizer. The same `save_pretrained` method is available for the tokenizer, which will generate other set of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can actually see all the files the tokenizer needs in order to operate. Let's tell Mlflow that we need all thes files to run the model. First, we need to create the dictionary I mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib\n",
    "\n",
    "artifacts = { pathlib.Path(file).stem: os.path.join(model_path, file) \n",
    "             for file in os.listdir(model_path) \n",
    "             if not os.path.basename(file).startswith('.') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is this code doing?** It creates a dictionary with the name of the file (without the extenison) as the key and the full path as the value. Files that start with a dot (.) are not included since usually this files are hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So artifacts now is a dictionary that contains all the elements we need to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How this artifacts will be loaded?\n",
    "\n",
    "We now need to tell Mlflow how to load this artifacts on inference time. When we introduced the class `PythonModel` from Mlflow we mentioned that the existance of the method `load_context` but we didn't say much more than that. We didn't implemented it in the Python wrapper we created. However, this method provides a chance for the model builder to load any artifacts that the model may need. Such artifacts are located inside the model package and can be accessed directly.\n",
    "\n",
    "In our case, we need to load the BERT model and the tokenizer. `transformers` library has a method `from_pretrained` that can handle models stored locally. We are going to use this inside of the `load_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.pyfunc import PythonModel, PythonModelContext\n",
    "from typing import Dict\n",
    "\n",
    "class BertTextClassifier(PythonModel):\n",
    "    def load_context(self, context: PythonModelContext):\n",
    "        import os\n",
    "        from transformers.models.auto import AutoConfig, AutoModelForSequenceClassification\n",
    "        from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "        \n",
    "        config_file = os.path.dirname(context.artifacts[\"config\"])\n",
    "        self.config = AutoConfig.from_pretrained(config_file)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config_file)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(config_file, config=self.config)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print('[INFO] Model is being sent to CUDA device as GPU is available')\n",
    "            self.model = self.model.cuda()\n",
    "        else:\n",
    "            print('[INFO] Model will use CPU runtime')\n",
    "        \n",
    "        _ = self.model.eval()\n",
    "        \n",
    "    def _predict_batch(self, data):\n",
    "        import torch\n",
    "        import pandas as pd\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(list(data['text'].values), padding=True, return_tensors='pt', truncation=True)\n",
    "        \n",
    "            if self.model.device.index != None:\n",
    "                torch.cuda.empty_cache()\n",
    "                for key in inputs.keys():\n",
    "                    inputs[key] = inputs[key].to(self.model.device.index)\n",
    "\n",
    "            predictions = self.model(**inputs)\n",
    "            probs = torch.nn.Softmax(dim=1)(predictions.logits)\n",
    "            probs = probs.detach().cpu().numpy()\n",
    "\n",
    "            classes = probs.argmax(axis=1)\n",
    "            confidences = probs.max(axis=1)\n",
    "\n",
    "            return classes, confidences\n",
    "        \n",
    "    def predict(self, context: PythonModelContext, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        import math\n",
    "        import numpy as np\n",
    "        \n",
    "        batch_size = 64\n",
    "        sample_size = len(data)\n",
    "        \n",
    "        classes = np.zeros(sample_size)\n",
    "        confidences = np.zeros(sample_size)\n",
    "\n",
    "        for batch_idx in range(0, math.ceil(sample_size / batch_size)):\n",
    "            bfrom = batch_idx * batch_size\n",
    "            bto = bfrom + batch_size\n",
    "            \n",
    "            c, p = self._predict_batch(data.iloc[bfrom:bto])\n",
    "            classes[bfrom:bto] = c\n",
    "            confidences[bfrom:bto] = p\n",
    "            \n",
    "        return pd.DataFrame({'rating': [self.config.id2label[c] for c in classes], \n",
    "                             'confidence': confidences })  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here a couple of things:\n",
    "- `context.artifacts` contains a dictionary similar to the one we created before, where value contains the path - **now inside the MLflow package** - where the asset `key` is located. So we can access any file directly. In this case, we are accesing the file `config`.\n",
    "- `transformers` library can load a mode, tokenizer and config directly from a folder, since it will then load each of the required files. This is why we are using just `artifacts['config']` path, although we have the path of the rest of the files also available (`artifacts['tokenizer']`, `artifacts['vocab']`, etc). We are actually extracting just the folder where the file is. However, in just case you may need to access each file individually.\n",
    "- `BertTextClassifier` doesn't have a constructor. This is not required, but since we are not using it I removed it. Use parameters in the constructor to indicate values that you want to persist with you model, but you don't have them on an artifact. For instance, the max lenght of the supported sequence, error messages values, or any other piece of data that you may need.\n",
    "- Imports are done always inside the `load_context` function or `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the pieces, it's time to log the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('bert-classification')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model('classifier', \n",
    "                            python_model=BertTextClassifier(), \n",
    "                            artifacts=artifacts, \n",
    "                            signature=signature,\n",
    "                            registered_model_name='bert-rating-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testig the MLFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model from the code using the following line. In this case we are assuming the model was registered using the name bert-rating-classification. We are also retrieving the last version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model = mlflow.pyfunc.load_model('models:/bert-rating-classification/latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the `predict` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the model locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the model in an inference server locally in our local compute. Again, with this we can check that our deployment strategy will work. \n",
    "\n",
    "To do so, let's serve our model using mlflow:\n",
    "\n",
    "```bash\n",
    "mlflow models serve -m models:/bert-rating-classification/latest\n",
    "```\n",
    "\n",
    "Creating a sample request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"sample.json\", \"w\") as f:\n",
    "    f.write(sample.to_json(orient='split', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the model inputs is indicated. MLFlow requires the inputs to the model to be submitted using `JSON` format and multiple specification are supported. In the Cats vs Dogs sample we saw before we used the TensorFlow Serving specification. Now, since we are using tabular data, we can use the Columnar format in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -A sample.json | curl http://127.0.0.1:5000/invocations \\\n",
    "                        --request POST \\\n",
    "                        --header 'Content-Type: application/json' \\\n",
    "                        --data-binary @-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_deploy_client(os.environ['MLFLOW_TRACKING_URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "deploy_config = {\n",
    "  \"computeType\": \"aci\",\n",
    "  \"containerResourceRequirements\": \n",
    "  {\n",
    "    \"cpu\": 2,\n",
    "    \"memoryInGB\": 4 \n",
    "  }\n",
    "}\n",
    "\n",
    "deployment_config_path = \"deployment_config.json\"\n",
    "with open(deployment_config_path, \"w\") as outfile:\n",
    "    outfile.write(json.dumps(deploy_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webservice = client.create_deployment(model_uri=f'models:/bert-rating-classification/latest',\n",
    "                                      name=\"bert-rating-classification\",\n",
    "                                      config={'deploy-config-file': deployment_config_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a sample request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"sample.json\", \"w\") as f:\n",
    "    f.write('{ \"input_data\": ' + sample.to_json(orient='split') + '}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the model inputs is indicated. MLFlow requires the inputs to the model to be submitted using `JSON` format and multiple specification are supported. In the Cats vs Dogs sample we saw before we used the TensorFlow Serving specification. Now, since we are using tabular data, we can use the Columnar format in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -A sample.json | curl http://f378d2e3-f044-44bd-9009-6a35abe4d78d.eastus.azurecontainer.io/score \\\n",
    "                    --request POST \\\n",
    "                    --header 'Content-Type: application/json' \\\n",
    "                    --header 'Authorization: Bearer <TOKEN>' \\\n",
    "                    --data-binary @-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Logging the model using a model loader\n",
    "\n",
    "As we saw, `artifacts` provide a convenient way to tell the model exacly what we need to run the model. However, it worth mentioning another alternative for those models that may require a couple of files to be executed, but we are fine having all of them in a folder an then load the entire directory with all that there is inside. \n",
    "\n",
    "This is the case of the transformers model we are working with, cause we can place all the files (tokenizer, model, vocab) in a folder and the library will just load what it needs. If this is the case, we can use model loaders (similar to what we did in the example #2 of the blog series. Just a couple of things would need to be changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile huggingface_model_loader.py\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.models.auto import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "\n",
    "class BertTextClassifier:\n",
    "    def __init__(self, baseline_model: str, tokenizer = None):\n",
    "        self.baseline_model = baseline_model\n",
    "        self.config = AutoConfig.from_pretrained(baseline_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer or baseline_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(baseline_model, config=self.config)\n",
    "        \n",
    "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        inputs = self.tokenizer(list(data['text'].values), padding=True, return_tensors='pt')\n",
    "        predictions = self.model(**inputs)\n",
    "        probs = torch.nn.Softmax(dim=1)(predictions.logits)\n",
    "        probs = probs.detach().numpy()\n",
    "        \n",
    "        classes = probs.argmax(axis=1)\n",
    "        confidences = probs.max(axis=1)\n",
    "        \n",
    "        return pd.DataFrame({'rating': [self.config.id2label[c] for c in classes], \n",
    "                             'confidence': confidences })\n",
    "        \n",
    "        \n",
    "def _load_pyfunc(path):\n",
    "    import os\n",
    "    return BertTextClassifier(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('bert-classification')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\"classifier\", \n",
    "                            data_path=model_path, \n",
    "                            code_path=[\"./huggingface_model_loader.py\"], \n",
    "                            loader_module=\"huggingface_model_loader\", \n",
    "                            registered_model_name=\"bert-rating-classification\", \n",
    "                            signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Both implementations are equally capable. We can decide which one is simpler depending on the scenario and requirements."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_pt_tf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('azureml_py38_PT_TF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9169f1d4e16acc976bbb73e323b0dbdf23f1c55e833fb2befffc4fb50ac2de2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
